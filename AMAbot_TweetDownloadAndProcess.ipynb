{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkTFZTJizGAO",
        "outputId": "8a38acee-907c-41b7-968d-d27205af90be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading @karpathy tweets... This should take no more than a minute!\n",
            "\n",
            "3242 tweets from @karpathy downloaded!\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import json\n",
        "import urllib3\n",
        "import pathlib\n",
        "import shutil\n",
        "import requests\n",
        "import os\n",
        "import re\n",
        "import random\n",
        "import code\n",
        "\n",
        "\n",
        "ALLOW_NEW_LINES =  False\n",
        "\n",
        "# ==== The following functons are from [@borisdayma](https://twitter.com/borisdayma)'s HuggingTweets demo, with modifications.\n",
        "def fix_text(text):\n",
        "    text = text.replace('&amp;', '&')\n",
        "    text = text.replace('&lt;', '<')\n",
        "    text = text.replace('&gt;', '>')\n",
        "    return text\n",
        "\n",
        "def clean_tweet(tweet, allow_new_lines = ALLOW_NEW_LINES):\n",
        "    bad_start = ['http:', 'https:']\n",
        "    for w in bad_start:\n",
        "        tweet = re.sub(f\" {w}\\\\S+\", \"\", tweet)      # removes white space before url\n",
        "        tweet = re.sub(f\"{w}\\\\S+ \", \"\", tweet)      # in case a tweet starts with a url\n",
        "        tweet = re.sub(f\"\\n{w}\\\\S+ \", \"\", tweet)    # in case the url is on a new line\n",
        "        tweet = re.sub(f\"\\n{w}\\\\S+\", \"\", tweet)     # in case the url is alone on a new line\n",
        "        tweet = re.sub(f\"{w}\\\\S+\", \"\", tweet)       # any other case?\n",
        "    tweet = re.sub(' +', ' ', tweet)                # replace multiple spaces with one space\n",
        "    if not allow_new_lines:                         # TODO: predictions seem better without new lines\n",
        "        tweet = ' '.join(tweet.split())\n",
        "    return tweet.strip()\n",
        "    \n",
        "def boring_tweet(tweet):\n",
        "    \"Check if this is a boring tweet\"\n",
        "    boring_stuff = ['http', '@', '#']\n",
        "    not_boring_words = len([None for w in tweet.split() if all(bs not in w.lower() for bs in boring_stuff)])\n",
        "    # return not_boring_words < 3  # Original\n",
        "    return not_boring_words < 10 or '@' in tweet or 'http:' in tweet or 'https:' in tweet # Modified\n",
        "\n",
        "# Max tweet length: 280\n",
        "# handle = 'FINALLEVEL' # 3206 tweets, 1522 final\n",
        "# handle= \"tszzl\" # 952 final. Some are long tweets.\n",
        "# handle = 'ThomasMiconi' # 2340 tweets, 506 final\n",
        "# handle = 'dril' # 3199 tweets, 1718 final\n",
        "handle = 'karpathy' # 3242 tweets, 1096 final, mean length 165 (without selecting on length<284: 1254 final, mean length 175)\n",
        "# handle = 'realdonaldtrump' # 3165 tweets, 904 final, mean length 176\n",
        "# handle = 'cher' #3197 tweets, 536 final... meanlength  193\n",
        "print(f'\\nDownloading @{handle} tweets... This should take no more than a minute!')\n",
        "http = urllib3.PoolManager(retries=urllib3.Retry(3))\n",
        "res = http.request(\"GET\", f\"http://us-central1-huggingtweets.cloudfunctions.net/get_tweets?handle={handle}&force=1\")\n",
        "res = json.loads(res.data.decode('utf-8'))\n",
        "\n",
        "user_name = res['user_name']\n",
        "all_tweets = res['tweets']\n",
        "\n",
        "raw_tweets  = all_tweets\n",
        "curated_tweets = [tweet for tweet in raw_tweets if len(tweet) < 284]\n",
        "fixed_tweets = [fix_text(tweet) for tweet in curated_tweets]\n",
        "print(f\"\\n{res['n_tweets']} tweets from @{handle} downloaded!\\n\\n\")\n",
        "\n",
        "# create dataset\n",
        "clean_tweets = [clean_tweet(tweet) for tweet in fixed_tweets]\n",
        "cool_tweets  = [tweet for tweet in clean_tweets if not boring_tweet(tweet)]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(res.keys())\n",
        "print(len(cool_tweets), \"tweets available after curation and filtering\")\n",
        "tweet_lengths = [len(x) for x in cool_tweets]\n",
        "print(\"Min / mean / max tweet length:\", min(tweet_lengths), sum(tweet_lengths) / len(tweet_lengths), max(tweet_lengths))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yM3gNCGu0WxR",
        "outputId": "d7127ba0-0d4b-4f7b-fe66-59f45af3d131"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['tweets', 'n_tweets', 'n_RT', 'n_kept', 'social_link', 'user_name', 'user_profile', 'wandb'])\n",
            "1096 tweets available after curation and filtering\n",
            "Min / mean / max tweet length: 46 165.0 280\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for tweet  in cool_tweets[2:22]:\n",
        "    print(tweet)\n",
        "    print(\"==\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VaZXEHTK0luB",
        "outputId": "24a8f3e3-43e4-4c0f-dde5-4ab009cab98d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wow, very nice \"full-stack\" release (again!) Allows finetuning of models as strong as LLaMA-65B on a single GPU as small as 48GB, in hours.\n",
            "==\n",
            "[New Talk] Pleasure to come by Microsoft BUILD this year and give a talk on \"State of GPT\". Goes through the GPT Assistant training pipeline, covers some \"LLM Psychology\", and offers a few best practices:\n",
            "==\n",
            "Someone has to redo that meme with the statistician vs deep learning “stack more layers” clown because the picture is shifting by one\n",
            "==\n",
            "Overheard: “People who know nothing about machine learning are now paradoxically advantaged in LLMs because they don’t immediately reach for overly sophisticated ideas and spend a lot more time hacking prompts” When hacking prompts feels below your dignity but it works :’|\n",
            "==\n",
            "Also highly relevant: guidance from microsoft \"Guidance programs allow you to interleave generation, prompting, and logical control\" Also internally handles subtle but important tokenization-related issues, e.g. \"token healing\".\n",
            "==\n",
            "Prompt: \"Give a 30 min talk on LLMs\" Me: 1 week and 170 slides later... 😵‍💫\n",
            "==\n",
            "You can take almost all brain uploading sci-fi and ideas and change them from 20+ years away (maybe) to small few years away (very likely) just by replacing occurrences of \"brain scanning\" with \"LLM finetuning\", and fidelity from ~perfect to lossy.\n",
            "==\n",
            "Full Stack LLM Bootcamp 8 lectures, high quality tokens 👍\n",
            "==\n",
            "Normalize light mode, dark mode, sci-fi mode. Must include rotating shapes\n",
            "==\n",
            "Random note on k-Nearest Neighbor lookups on embeddings: in my experience much better results can be obtained by training SVMs instead. Not too widely known. Short example: Works because SVM ranking considers the unique aspects of your query w.r.t. data.\n",
            "==\n",
            "I wonder if von Neumann had a large d_model, n_layer, head_size or block_size, or kv cache. All of these hyperparams might manifest slightly different.\n",
            "==\n",
            "All of that is just one agent/thread. People coalesce into organizations so they can specialize and parallelize work towards shared goals. Imo this is likely to happen to AutoGPTs and for the same reasons, strung into AutoOrgs, with AutoCEO, AutoCFO, AutoICs, etc.\n",
            "==\n",
            "(so I'd expect the good prompts to explicitly address things like this)\n",
            "==\n",
            "Interesting non-obvious note on GPT psychology is that unlike people they are completely unaware of their own strengths and limitations. E.g. that they have finite context window. That they can just barely do mental math. That samples can get unlucky and go off the rails. Etc.\n",
            "==\n",
            "Tired: write comments to prompt copilot to write code. Wired: just write comments. it's cleaner :D\n",
            "==\n",
            "Good example of us not seeing max GPT-4 capability yet, imo. Prompt design, tool use, meta cognition strategies (eg idea of attempt, critique, retry, capabilities model, etc) are very likely to go a long way.\n",
            "==\n",
            "\"How to chat with a 56-page PDF\" Good developer-focused YouTube explainer: Very excited about the growing layer of software infrastructure on top of GPT APIs, and all of the possible extensions here.\n",
            "==\n",
            "The vibes when I joined AI in ~2008: - workshops w 50 ppl musing on whether deep learning will ever work - papers w cute toy problems - fun poster sessions - this experiment I ran in MATLAB - high-level panels on paths to AI - neuroscience guest lectures Today is *not* the same.\n",
            "==\n",
            "GPT is a new kind of computer architecture that runs on text. Yes it can talk to us, but also to much of our existing software infrastructure. First via apps on top of APIs, now inside ChatGPT via plugins. What a time right now...\n",
            "==\n",
            "Plot twist John Connor is not a soldier but a prompt engineer\n",
            "==\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"tweets_\"+str(handle)+\".txt\",'w',encoding='utf8') as f:\n",
        "    for tweet in cool_tweets:\n",
        "        f.write(tweet+\"\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "JYYYSFGo0738"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_E6cYThv6ren"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}